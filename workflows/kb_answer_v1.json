[{
    "updatedAt": "2025-12-19T09:23:22.250Z",
    "createdAt": "2025-12-17T14:18:50.698Z",
    "id": "8GCA2MiYJ0R64Ort",
    "name": "KB — Answer v1",
    "description": null,
    "active": true,
    "isArchived": false,
    "nodes": [
      {
        "parameters": {
          "inputSource": "passthrough"
        },
        "type": "n8n-nodes-base.executeWorkflowTrigger",
        "typeVersion": 1.1,
        "position": [
          80,
          -208
        ],
        "id": "335a7169-9521-4b89-9a5c-e39fed785db3",
        "name": "When Executed by Another Workflow"
      },
      {
        "parameters": {
          "mode": "runOnceForEachItem",
          "jsCode": "const body = $json?.body ?? null;\nconst payload = (body && typeof body === \"object\") ? body : $json;\n\n// Promote webhook body fields to top-level so DB query / Gate logic can read them.\nconst merged = (body && typeof body === \"object\") ? { ...$json, ...body } : { ...$json };\n\nconst qRaw =\n  payload?.input?.question ??\n  payload?.question ??\n  payload?.text ??\n  payload?.telegram?.message?.text ??\n  \"\";\n\nconst question = String(qRaw).trim();\n\nconst kbRefRaw =\n  payload?.meta?.kb_ref ??\n  payload?.kb_ref ??\n  merged?.meta?.kb_ref ??\n  merged?.kb_ref ??\n  \"\";\n\nconst kb_ref = String(kbRefRaw).trim() || null;\n\nconst retrievalIn =\n  (payload?.retrieval && typeof payload.retrieval === \"object\") ? payload.retrieval :\n  (merged?.retrieval && typeof merged.retrieval === \"object\") ? merged.retrieval :\n  {};\n\nconst topKRaw = retrievalIn?.top_k ?? retrievalIn?.topK ?? payload?.top_k ?? payload?.topK ?? null;\nconst top_k = topKRaw == null ? 5 : Number(topKRaw);\n\nconst retrieval = {\n  ...retrievalIn,\n  top_k: Number.isFinite(top_k) && top_k > 0 ? top_k : 5,\n};\n\nconst chatIdRaw =\n  payload?.meta?.chat_id ??\n  payload?.chat_id ??\n  payload?.telegram?.message?.chat?.id ??\n  payload?.input?.user_id ??\n  merged?.meta?.chat_id ??\n  merged?.chat_id ??\n  merged?.telegram?.message?.chat?.id ??\n  merged?.input?.user_id ??\n  null;\n\nconst chat_id = chatIdRaw == null ? null : Number(chatIdRaw);\n\nconst request_id =\n  payload?.meta?.request_id ??\n  payload?.request_id ??\n  merged?.meta?.request_id ??\n  merged?.request_id ??\n  null;\n\nconst metaIn = (payload?.meta && typeof payload.meta === \"object\") ? payload.meta : (merged?.meta ?? {});\nconst inputIn = (payload?.input && typeof payload.input === \"object\") ? payload.input : (merged?.input ?? {});\n\nconst meta = {\n  ...metaIn,\n  kb_ref: metaIn?.kb_ref ?? kb_ref,\n};\n\nconst input = {\n  ...inputIn,\n  question: inputIn?.question ?? question,\n};\n\nreturn {\n  json: {\n    ...merged,\n    kb_ref,\n    retrieval,\n    meta,\n    input,\n    question,\n    chat_id: Number.isFinite(chat_id) ? chat_id : null,\n    request_id,\n  },\n};"
        },
        "type": "n8n-nodes-base.code",
        "typeVersion": 2,
        "position": [
          288,
          -208
        ],
        "id": "f66ded67-36ee-46d7-a77c-9e3a0d1058c6",
        "name": "KB — Input Normalize"
      },
      {
        "parameters": {
          "method": "POST",
          "url": "https://api.openai.com/v1/embeddings",
          "authentication": "predefinedCredentialType",
          "nodeCredentialType": "openAiApi",
          "sendBody": true,
          "specifyBody": "json",
          "jsonBody": "={\n  \"model\": \"text-embedding-3-small\",\n  \"input\": \"={{$json.question}}\"\n}\n",
          "options": {
            "timeout": 6000
          }
        },
        "type": "n8n-nodes-base.httpRequest",
        "typeVersion": 4.3,
        "position": [
          480,
          -208
        ],
        "id": "c55f5af8-78ed-49cb-ab4e-85bbfa9e1b92",
        "name": "OpenAI — Embeddings (HTTP)",
        "credentials": {
          "openAiApi": {
            "id": "j2qEVgJkVL5JMOK6",
            "name": "OpenAi account"
          }
        }
      },
      {
        "parameters": {
          "mode": "runOnceForEachItem",
          "jsCode": "const idx = $itemIndex;\nconst env = $items(\"KB — Input Normalize\")?.[idx]?.json ?? {};\nconst emb = $json.data?.[0]?.embedding;\nif (!emb) throw new Error(\"No embedding in OpenAI response\");\n\nreturn {\n  json: {\n    ...env,\n    query_embedding: emb,\n    meta: {\n      ...(env.meta ?? {}),\n      embedding_model: $json.model ?? \"text-embedding-3-small\",\n    },\n    timers: {\n      ...(env.timers ?? {}),\n      t_after_embed_ms: Date.now(),\n    },\n  },\n};\n"
        },
        "type": "n8n-nodes-base.code",
        "typeVersion": 2,
        "position": [
          656,
          -208
        ],
        "id": "469d3e35-5898-4aac-bf45-e70070791971",
        "name": "KB — Extract Query Embedding"
      },
      {
        "parameters": {
          "operation": "executeQuery",
          "query": "with p as (select $1::jsonb as j),\nscored as (\n  select\n    k.chunk_id, k.kb_ref, k.doc, k.section, k.chunk_index, k.content, k.source_url,\n    (k.embedding <-> (p.j->>'query_embedding')::vector) as distance,\n    row_number() over (order by k.embedding <-> (p.j->>'query_embedding')::vector) as rn,\n    coalesce((p.j->'retrieval'->>'top_k')::int, 5) as top_k\n  from public.kb_chunks k\n  cross join p\n  where k.kb_ref = coalesce(p.j->>'kb_ref', 'demo_support')\n)\nselect\n  chunk_id, kb_ref, doc, section, chunk_index, content, source_url,\n  distance,\n  1.0 / (1.0 + distance) as score\nfrom scored\nwhere rn <= top_k\norder by rn;\n",
          "options": {
            "connectionTimeout": 6,
            "queryReplacement": "={{ JSON.stringify($json) }}"
          }
        },
        "type": "n8n-nodes-base.postgres",
        "typeVersion": 2.6,
        "position": [
          816,
          -208
        ],
        "id": "64a7f4eb-5f32-4d8a-a622-5683390f199c",
        "name": "DB — Retrieve kb_chunks (topK)",
        "credentials": {
          "postgres": {
            "id": "QLUbzmxXFGSZ8LHx",
            "name": "AI_Support_Assistant"
          }
        }
      },
      {
        "parameters": {
          "jsCode": "// items = строки из DB (каждая строка — отдельный item)\nconst rows = items\n  .map(i => i.json)\n  .filter(r => r && r.content);\n\n// подтягиваем исходный Envelope (из ноды перед DB)\nconst env = $node[\"KB — Extract Query Embedding\"]?.json ?? {};\n\n// сортируем по score (чем больше — тем релевантнее)\nrows.sort((a, b) => (b.score ?? 0) - (a.score ?? 0));\n\nconst MAX_CHARS_PER_CHUNK = 1400;\n\nconst kb_sources = rows.map((r, idx) => ({\n  n: idx + 1,\n  chunk_id: r.chunk_id ?? null,\n  doc: r.doc ?? null,\n  section: r.section ?? null,\n  source_url: r.source_url ?? null,\n  score: r.score ?? null,\n  distance: r.distance ?? null,\n}));\n\nconst kb_context_text = rows.map((r, idx) => {\n  let content = String(r.content ?? \"\").trim();\n  if (content.length > MAX_CHARS_PER_CHUNK) {\n    content = content.slice(0, MAX_CHARS_PER_CHUNK) + \"…\";\n  }\n  const scoreStr = (r.score ?? 0).toFixed(3);\n  return `[${idx + 1}] doc=\"${r.doc ?? \"\"}\" section=\"${r.section ?? \"\"}\" score=${scoreStr}\n${content}`;\n}).join(\"\n\n\");\n\n// обновим retrieval в envelope\nconst top_score = rows[0]?.score ?? null;\n\nreturn [{\n  json: {\n    ...env,\n    retrieval: {\n      ...(env.retrieval ?? {}),\n      hits_count: rows.length,\n      top_score,\n      hits: kb_sources,\n      compiled_context: kb_context_text,\n    },\n    kb_context_text,\n    kb_sources,\n  }\n}];\n"
        },
        "type": "n8n-nodes-base.code",
        "typeVersion": 2,
        "position": [
          976,
          -208
        ],
        "id": "68396f44-aa53-49a4-aa60-c0539b11a3d3",
        "name": "KB — Build Context"
      },
      {
        "parameters": {
          "conditions": {
            "options": {
              "caseSensitive": true,
              "leftValue": "",
              "typeValidation": "strict",
              "version": 3
            },
            "conditions": [
              {
                "id": "88b64325-a1e5-4c10-9b78-4446f2523d48",
                "leftValue": "={{$json.decision.mode}}",
                "rightValue": "ALLOW",
                "operator": {
                  "type": "string",
                  "operation": "equals",
                  "name": "filter.operator.equals"
                }
              }
            ],
            "combinator": "and"
          },
          "options": {}
        },
        "type": "n8n-nodes-base.if",
        "typeVersion": 2.3,
        "position": [
          1280,
          -208
        ],
        "id": "74ebcdcf-cb94-4c45-a065-f113fb1bf7a1",
        "name": "If"
      },
      {
        "parameters": {
          "jsCode": "const topScore =\n  Number($json?.retrieval?.top_score ?? $json?.retrieval?.topScore ?? 0);\n\nconst hitsCount =\n  Number($json?.retrieval?.hits_count\n    ?? $json?.retrieval?.hitsCount\n    ?? $json?.retrieval?.hits?.length\n    ?? $json?.hits_count\n    ?? 0);\n\nconst cfg = {\n  minHits: 1,\n  tClarify: 0.46, // T1\n  tAllow: 0.52    // T2\n};\n\nlet mode = \"NO_ANSWER\";\nlet reason = \"no_hits\";\n\nif (hitsCount < cfg.minHits) {\n  mode = \"NO_ANSWER\";\n  reason = \"no_hits\";\n} else if (topScore >= cfg.tAllow) {\n  mode = \"ALLOW\";\n  reason = \"ok\";\n} else if (topScore >= cfg.tClarify) {\n  mode = \"CLARIFY\";\n  reason = \"low_confidence\";\n} else {\n  mode = \"NO_ANSWER\";\n  reason = \"low_similarity\";\n}\n\n// For NON-ALLOW we immediately set a user-facing fallback message in output.answer\n// so the parent workflow can just Send Message + Log (no LLM call).\nlet output = $json.output ?? {};\nif (mode !== \"ALLOW\") {\n  if (mode === \"CLARIFY\") {\n    output.answer =\n      \"I found partially relevant info in the knowledge base, but I need a bit more detail to answer.\\n\\n\" +\n      \"What to clarify:\\n\" +\n      \"- Which exact section/process are you referring to (function/page/step name)?\\n\" +\n      \"- Which system/integration is this about (if there are multiple)?\";\n  } else {\n    output.answer =\n      \"I couldn’t find an answer in the knowledge base for this question.\\n\\n\" +\n      \"What you can do:\\n\" +\n      \"- Rephrase the question\\n\" +\n      \"- Add 1–2 details (feature/section name, error code, step in the process)\";\n  }\n  output.sources = [];\n}\n\nreturn [{\n  json: {\n    ...$json,\n    decision: { ...($json.decision ?? {}), mode, reason },\n    output\n  }\n}];\n"
        },
        "type": "n8n-nodes-base.code",
        "typeVersion": 2,
        "position": [
          1136,
          -208
        ],
        "id": "9d58433d-556d-42be-a6fa-c9093ed43d07",
        "name": "Gate Decide"
      },
      {
        "parameters": {
          "jsCode": "return items.map(item => {\n  const j = item.json;\n\n  const picked = Array.isArray(j.output?.picked_hits) ? j.output.picked_hits : [];\n  const sources = picked\n    .map((h, i) => ({\n      n: h.n ?? (i + 1),\n      chunk_id: h.chunk_id ?? null,\n      doc: h.doc ?? \"\",\n      section: h.section ?? \"\",\n      source_url: h.source_url ?? null,\n      score: Number(h.score ?? 0)\n    }))\n    .sort((a, b) => b.score - a.score)\n    .slice(0, 5);\n\n  j.output = j.output ?? {};\n  j.output.sources = sources;\n\n  return { json: j };\n});"
        },
        "type": "n8n-nodes-base.code",
        "typeVersion": 2,
        "position": [
          2320,
          -368
        ],
        "id": "9b4981b9-8eb8-41df-a431-27fe60e84531",
        "name": "KB — Build Sources"
      },
      {
        "parameters": {
          "jsCode": "return items.map(item => {\n  const j = item.json;\n\n  const mode = String(j.decision?.mode ?? \"NO_ANSWER\").toUpperCase();\n  const answerText = (j.output?.answer_text ?? \"\").trim();\n  const clarify = Array.isArray(j.output?.clarify) ? j.output.clarify : [];\n  const sources = Array.isArray(j.output?.sources) ? j.output.sources : [];\n\n  let text = \"\";\n\n  if (mode === \"ALLOW\") {\n    text = answerText || \"OK.\";\n\n    if (sources.length) {\n      const sourcesBlock =\n        \"\\n\\nSources:\\n\" +\n        sources.map((s, i) => {\n          const title = `${s.doc}${s.section ? \" — \" + s.section : \"\"}`;\n          const url = s.source_url ? `\\n${s.source_url}` : \"\";\n          return `${i + 1}) ${title}${url}`;\n        }).join(\"\\n\\n\");\n\n      text += sourcesBlock;\n    }\n  } else if (mode === \"CLARIFY\") {\n    const qs = clarify.length ? clarify : [\"Please уточни вопрос.\"];\n    text = \"I need a bit more info:\\n\" + qs.map(x => `- ${x}`).join(\"\\n\");\n  } else {\n    text = \"I couldn't find the answer in the Knowledge Base.\";\n  }\n\n  j.output = j.output ?? {};\n  j.output.reply_text = text;\n  j.output.answer = text; // backward-compat\n\n  return { json: j };\n});"
        },
        "type": "n8n-nodes-base.code",
        "typeVersion": 2,
        "position": [
          2528,
          -368
        ],
        "id": "fa683efe-1aa1-4fa4-b21a-91fa6d647d21",
        "name": "KB — Build Final Reply"
      },
      {
        "parameters": {
          "jsCode": "return items.map(item => {\n  const j = item.json;\n\n  delete j.query_embedding;\n\n  if (j.retrieval) {\n    delete j.retrieval.compiled_context;\n    if (Array.isArray(j.retrieval.hits)) {\n      j.retrieval.hits = j.retrieval.hits.map(h => {\n        const hh = { ...h };\n        delete hh.embedding;\n        delete hh.embedding_vector;\n        return hh;\n      });\n    }\n  }\n\n  if (j.llm) {\n    delete j.llm.raw_text;\n  }\n\n  if (j.output) {\n    delete j.output.picked_hits;\n  }\n\n  return { json: j };\n});"
        },
        "type": "n8n-nodes-base.code",
        "typeVersion": 2,
        "position": [
          2736,
          -368
        ],
        "id": "47875140-f5bc-4c07-bf3d-515fb90fdeb6",
        "name": "KB — Strip Heavy Fields"
      },
      {
        "parameters": {
          "jsCode": "return items.map(item => {\n  const j = item.json;\n\n  j.decision = j.decision ?? {};\n  j.output = j.output ?? {};\n\n  const mode = String(j.decision.mode ?? \"NO_ANSWER\").toUpperCase();\n\n  // Keep fallback responses citation-free (sources are attached only for ALLOW).\n  j.output.sources = [];\n\n  if (mode === \"CLARIFY\") {\n    if (!String(j.output.answer ?? \"\").trim()) {\n      j.output.answer =\n        \"I found partially relevant info in the knowledge base, but I need a bit more detail to answer.\\n\\n\" +\n        \"What to clarify:\\n\" +\n        \"- Which exact section/process are you referring to (function/page/step name)?\\n\" +\n        \"- Which system/integration is this about (if there are multiple)?\";\n    }\n  } else {\n    if (!String(j.output.answer ?? \"\").trim()) {\n      j.output.answer =\n        \"I couldn’t find an answer in the knowledge base for this question.\\n\\n\" +\n        \"What you can do:\\n\" +\n        \"- Rephrase the question\\n\" +\n        \"- Add 1–2 details (feature/section name, error code, step in the process)\";\n    }\n  }\n\n  return { json: j };\n});"
        },
        "type": "n8n-nodes-base.code",
        "typeVersion": 2,
        "position": [
          1472,
          -112
        ],
        "id": "834669b3-ef6b-4681-b943-bb1e62475ccc",
        "name": "Ops — Finalize Fallback"
      },
      {
        "parameters": {
          "jsCode": "return items.map(item => {\n  const j = item.json;\n\n  delete j.query_embedding;\n\n  // обычно при fallback контекст/источники тоже не нужны дальше\n  // (оставь, если хочешь видеть в логах)\n  // delete j.kb_context_text;\n  // delete j.kb_sources;\n\n  return { json: j };\n});"
        },
        "type": "n8n-nodes-base.code",
        "typeVersion": 2,
        "position": [
          1648,
          -112
        ],
        "id": "c6a318df-42ae-479a-94f5-c5135d155b04",
        "name": "Ops — Strip Heavy Fields"
      },
      {
        "parameters": {
          "modelId": {
            "__rl": true,
            "value": "gpt-4.1-mini",
            "mode": "list",
            "cachedResultName": "GPT-4.1-MINI"
          },
          "responses": {
            "values": [
              {
                "role": "system",
                "content": "={{ $json.prompt_text }}"
              },
              {
                "content": "=Question:\n{{$json.input.question}}\n\nContext (KB chunks):\n{{$json.retrieval.compiled_context}}"
              }
            ]
          },
          "simplify": false,
          "builtInTools": {},
          "options": {
            "temperature": 0.2
          }
        },
        "type": "@n8n/n8n-nodes-langchain.openAi",
        "typeVersion": 2.1,
        "position": [
          1808,
          -368
        ],
        "id": "e0584636-4cf1-449b-ae85-c8775ea667b7",
        "name": "OpenAI — Answer v1",
        "retryOnFail": true,
        "credentials": {
          "openAiApi": {
            "id": "j2qEVgJkVL5JMOK6",
            "name": "OpenAi account"
          }
        },
        "onError": "continueRegularOutput"
      },
      {
        "parameters": {
          "jsCode": "const base = $node[\"Gate Decide\"].json;\n\nfunction stripCodeFences(s) {\n  return String(s ?? \"\")\n    .trim()\n    .replace(/^```(?:json)?\\s*/i, \"\")\n    .replace(/\\s*```$/i, \"\")\n    .trim();\n}\n\nfunction extractResponsesOutputText(resp) {\n  const out = resp?.output;\n  if (Array.isArray(out)) {\n    for (const item of out) {\n      if (item?.type === \"message\" && Array.isArray(item?.content)) {\n        for (const c of item.content) {\n          if (c?.type === \"output_text\" && typeof c?.text === \"string\" && c.text.trim()) {\n            return c.text.trim();\n          }\n        }\n        const t2 = item.content?.[0]?.text;\n        if (typeof t2 === \"string\" && t2.trim()) return t2.trim();\n      }
    }
  }
  return \"\";
}

function extractChatCompletionsText(resp) {
  const cc =
    resp?.choices?.[0]?.message?.content ??
    resp?.data?.choices?.[0]?.message?.content ??
    resp?.message?.content;
  if (typeof cc === \"string\" && cc.trim()) return cc.trim();
  return \"\";
}

function extractResponseText(resp) {
  const t = extractResponsesOutputText(resp);
  if (t) return t;
  const cc = extractChatCompletionsText(resp);
  if (cc) return cc;
  if (typeof resp === \"string\" && resp.trim()) return resp.trim();
  return \"\";
}

function parseJsonLoose(raw) {
  const s = stripCodeFences(raw);
  try {
    const p = JSON.parse(s);
    if (typeof p === \"string\") return JSON.parse(stripCodeFences(p));
    return p;
  } catch {}
  const m = s.match(/\{[\s\S]*\}/);
  if (m) {
    try {
      return JSON.parse(m[0]);
    } catch {}
  }
  return null;
}

function normalizeMode(m) {
  const x = String(m ?? \"\").trim().toUpperCase();
  if (x === \"ANSWER\") return \"ALLOW\";
  if (x === \"ALLOW\") return \"ALLOW\";
  if (x === \"CLARIFY\") return \"CLARIFY\";
  if (x === \"NO_ANSWER\" || x === \"NOANSWER\" || x === \"NO-ANSWER\") return \"NO_ANSWER\";
  return \"\";
}

function ensureArray(x) {
  if (Array.isArray(x)) return x;
  if (typeof x === \"string\" && x.trim()) return [x.trim()];
  return [];
}

function pickHits({ parsed, answerText, hits, topScore }) {
  const byN = new Map();
  const byId = new Map();
  for (const h of hits) {
    if (h?.n != null) byN.set(Number(h.n), h);
    if (h?.chunk_id) byId.set(String(h.chunk_id), h);
  }

  const explicit = parsed?.sources;
  if (Array.isArray(explicit) && explicit.length) {
    const picked = [];
    for (const x of explicit) {
      if (typeof x === \"number\" || /^\d+$/.test(String(x))) {
        const h = byN.get(Number(x));
        if (h) picked.push(h);
      } else {
        const h = byId.get(String(x));
        if (h) picked.push(h);
      }
    }
    if (picked.length) return picked;
  }

  const cited = new Set();
  const re = /\[(\d+)\]/g;
  let m;
  while ((m = re.exec(String(answerText ?? \"\"))) !== null) cited.add(Number(m[1]));
  if (cited.size) return [...cited].map((n) => byN.get(n)).filter(Boolean);

  const ts = typeof topScore === \"number\" ? topScore : Number(hits?.[0]?.score ?? 0);
  const minScore = ts * 0.9;
  return hits.filter((h) => Number(h?.score ?? 0) >= minScore);
}

// вытаскиваем текст первого источника из kb_context_text
function extractTopSourceText(kbContext) {
  const s = String(kbContext ?? \"\");
  const m = s.match(/\[1\][^\n]*\n([\s\S]*?)(?:\n\n\[\d+\]|\s*$)/);
  if (!m) return \"\";
  return String(m[1] ?? \"\").trim();
}

function extractModel(resp) {
  const direct = [resp?.model, resp?.data?.model, resp?.info?.model, resp?.metadata?.model];
  for (const m of direct) {
    if (typeof m === \"string\" && m.trim()) return m.trim();
  }

  const choices = resp?.choices ?? resp?.data?.choices;
  if (Array.isArray(choices)) {
    for (const c of choices) {
      const m = c?.model ?? c?.message?.model ?? c?.completion?.model;
      if (typeof m === \"string\" && m.trim()) return m.trim();
    }
  }

  const nested = resp?.response ?? resp?.result ?? resp?.body;
  if (nested && nested !== resp) {
    const nestedModel = extractModel(nested);
    if (nestedModel) return nestedModel;
  }

  return \"\";
}

// ====== main ======
const raw = extractResponseText($json);
const parsed = parseJsonLoose(raw);

const hits = base?.kb_sources ?? base?.retrieval?.hits ?? [];
const topScore = base?.retrieval?.top_score ?? Number(hits?.[0]?.score ?? 0);

// gate decision (retrieval)
const gateMode = String(base?.decision?.mode ?? \"ALLOW\").toUpperCase();

// llm mode
let llmMode = normalizeMode(parsed?.mode);
let llmReason = \"ok\";

if (!llmMode) {
  llmMode = \"CLARIFY\";
  llmReason = \"llm_parse_error\";
}

let finalMode = llmMode;
let finalReason = llmReason;

let answerText = \"\";
let clarify = [];

// если gate НЕ ALLOW — просто не трогаем (как правило LLM тут вообще не должен вызываться)
if (gateMode !== \"ALLOW\") {
  finalMode = gateMode;
  finalReason = base?.decision?.reason ?? \"ok\";
}

// если gate ALLOW — решаем, что показывать пользователю
if (gateMode === \"ALLOW\") {
  if (llmMode === \"ALLOW\") {
    answerText = String(parsed?.answer ?? \"\").trim();
  } else if (llmMode === \"CLARIFY\") {
    clarify = ensureArray(parsed?.clarify);
  } else if (llmMode === \"NO_ANSWER\") {
    answerText = String(parsed?.answer ?? \"\").trim();
  }

  // OVERRIDE: если LLM сказал CLARIFY/NO_ANSWER, но top_score высокий и top source явно отвечает — отвечаем
  const FORCE_ANSWER_SCORE = 0.5; // “high enough” threshold
  if ((llmMode === \"CLARIFY\" || llmMode === \"NO_ANSWER\") && hits.length && topScore >= FORCE_ANSWER_SCORE) {
    const topText = extractTopSourceText(base?.kb_context_text);
    if (topText) {
      finalMode = \"ALLOW\";
      finalReason = \"llm_override_high_score\";
      answerText = topText;
      clarify = [];
    }
  }

  // если ALLOW, но пусто — переводим в CLARIFY как safe fallback
  if (finalMode === \"ALLOW\" && !String(answerText).trim()) {
    finalMode = \"CLARIFY\";
    finalReason = \"llm_empty_answer\";
    clarify = [\"What exact access do you need (system/app) and for which role?\"];
  }

  // если CLARIFY и пусто — дефолт
  if (finalMode === \"CLARIFY\" && (!Array.isArray(clarify) || !clarify.length)) {
    clarify = [\"What exact access do you need (system/app) and for which role?\"];
  }

  // если NO_ANSWER и пусто — дефолт
  if (finalMode === \"NO_ANSWER\" && !String(answerText).trim()) {
    answerText = \"I couldn't find the answer in the Knowledge Base.\";
  }
}

// sources только для ALLOW
let picked_hits = [];
if (finalMode === \"ALLOW\") {
  picked_hits = pickHits({ parsed, answerText, hits, topScore });
}

const detectedModel = extractModel($json);
const nextMeta = { ...(base.meta ?? {}) };

if (detectedModel) nextMeta.chat_model = detectedModel;
if (nextMeta.prompt_version == null && base?.meta?.prompt_version != null) {
  nextMeta.prompt_version = base.meta.prompt_version;
}

return [
  {
    json: {
      ...base,
      meta: nextMeta,
      // ✅ теперь decision = финальный режим (консистентно с ответом)
      decision: {
        ...(base.decision ?? {}),
        mode: finalMode,
        reason: finalReason,
      },
      output: {
        ...(base.output ?? {}),
        mode: finalMode,
        answer_text: finalMode === \"ALLOW\" ? String(answerText).trim() : \"\",
        clarify: finalMode === \"CLARIFY\" ? clarify : [],
        picked_hits,
      },
      llm: {
        raw_text: raw,
        parsed: parsed ?? null,
        model: detectedModel || null,
      },
    },
  },
];"
        },
        "type": "n8n-nodes-base.code",
        "typeVersion": 2,
        "position": [
          2112,
          -368
        ],
        "id": "c7c02f37-1142-481b-aad9-8d3b4e3a6346",
        "name": "KB — Parse Answer"
      },
      {
        "parameters": {
          "url": "https://raw.githubusercontent.com/ArsenYoung/ai-support-rag-assistant/main/prompts/answer.md",
          "options": {
            "response": {
              "response": {
                "responseFormat": "text"
              }
            }
          }
        },
        "type": "n8n-nodes-base.httpRequest",
        "typeVersion": 4.3,
        "position": [
          1472,
          -368
        ],
        "id": "2d8d32e9-1a40-48bd-aa03-fe613e66b903",
        "name": "Prompt — Load answer.md"
      },
      {
        "parameters": {
          "jsCode": "function fnv1a32(str) {\n  let h = 0x811c9dc5;\n  for (let i = 0; i < str.length; i++) {\n    h ^= str.charCodeAt(i);\n    h = Math.imul(h, 0x01000193);\n  }\n  return (\"00000000\" + (h >>> 0).toString(16)).slice(-8);\n}\n\nconst prompt_text = ($json.body ?? $json.data ?? $json.prompt ?? \"\").toString();\nif (!prompt_text.trim()) {\n  throw new Error(\"PROMPT_LOAD_ERROR: empty prompt_text\");\n}\n\nreturn [{\n  json: {\n    ...$json,\n    prompt_text,\n    meta: {\n      ...( $json.meta || {} ),\n      prompt_version: \"v1\",\n      prompt_hash: \"fnv1a32:\" + fnv1a32(prompt_text),\n    }\n  }\n}];"
        },
        "type": "n8n-nodes-base.code",
        "typeVersion": 2,
        "position": [
          1648,
          -368
        ],
        "id": "3fc3f75e-4d6d-4716-8342-27070cb38955",
        "name": "Prompt — Attach Meta"
      },
      {
        "parameters": {
          "workflowId": {
            "__rl": true,
            "value": "Z8yQeXYTQSnRkl3M",
            "mode": "list",
            "cachedResultUrl": "/workflow/Z8yQeXYTQSnRkl3M",
            "cachedResultName": "Ops — Log chat_turn"
          },
          "workflowInputs": {
            "mappingMode": "defineBelow",
            "value": {},
            "matchingColumns": [],
            "schema": [],
            "attemptToConvertTypes": false,
            "convertFieldsToString": true
          },
          "options": {
            "waitForSubWorkflow": false
          }
        },
        "type": "n8n-nodes-base.executeWorkflow",
        "typeVersion": 1.3,
        "position": [
          3264,
          -112
        ],
        "id": "32ac22c0-e821-4108-89c1-f264ae04f050",
        "name": "Call 'Ops — Log chat_turn'"
      },
      {
        "parameters": {
          "httpMethod": "POST",
          "path": "kb/answer",
          "responseMode": "responseNode",
          "options": {}
        },
        "type": "n8n-nodes-base.webhook",
        "typeVersion": 2.1,
        "position": [
          80,
          -368
        ],
        "id": "cb195b64-c1b5-470d-b6a3-536c65f6b1db",
        "name": "Webhook",
        "webhookId": "d3de4c60-5f27-430d-aa33-22ac35c89e58"
      },
      {
        "parameters": {
          "options": {
            "responseCode": 200
          }
        },
        "type": "n8n-nodes-base.respondToWebhook",
        "typeVersion": 1.5,
        "position": [
          2896,
          -208
        ],
        "id": "6b568123-beef-4e31-9c4f-afc8ade73689",
        "name": "Respond 200"
      }
    ],
    "connections": {
      "When Executed by Another Workflow": {
        "main": [
          [
            {
              "node": "KB — Input Normalize",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "KB — Input Normalize": {
        "main": [
          [
            {
              "node": "OpenAI — Embeddings (HTTP)",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "OpenAI — Embeddings (HTTP)": {
        "main": [
          [
            {
              "node": "KB — Extract Query Embedding",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "KB — Extract Query Embedding": {
        "main": [
          [
            {
              "node": "DB — Retrieve kb_chunks (topK)",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "DB — Retrieve kb_chunks (topK)": {
        "main": [
          [
            {
              "node": "KB — Build Context",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "KB — Build Context": {
        "main": [
          [
            {
              "node": "Gate Decide",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "If": {
        "main": [
          [
            {
              "node": "Prompt — Load answer.md",
              "type": "main",
              "index": 0
            }
          ],
          [
            {
              "node": "Ops — Finalize Fallback",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Gate Decide": {
        "main": [
          [
            {
              "node": "If",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "KB — Build Sources": {
        "main": [
          [
            {
              "node": "KB — Build Final Reply",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "KB — Build Final Reply": {
        "main": [
          [
            {
              "node": "KB — Strip Heavy Fields",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Ops — Finalize Fallback": {
        "main": [
          [
            {
              "node": "Ops — Strip Heavy Fields",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "OpenAI — Answer v1": {
        "main": [
          [
            {
              "node": "KB — Parse Answer",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "KB — Parse Answer": {
        "main": [
          [
            {
              "node": "KB — Build Sources",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Prompt — Load answer.md": {
        "main": [
          [
            {
              "node": "Prompt — Attach Meta",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Prompt — Attach Meta": {
        "main": [
          [
            {
              "node": "OpenAI — Answer v1",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Ops — Strip Heavy Fields": {
        "main": [
          [
            {
              "node": "Call 'Ops — Log chat_turn'",
              "type": "main",
              "index": 0
            },
            {
              "node": "Respond 200",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "KB — Strip Heavy Fields": {
        "main": [
          [
            {
              "node": "Call 'Ops — Log chat_turn'",
              "type": "main",
              "index": 0
            },
            {
              "node": "Respond 200",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Webhook": {
        "main": [
          [
            {
              "node": "KB — Input Normalize",
              "type": "main",
              "index": 0
            }
          ]
        ]
      }
    },
    "settings": {
      "executionOrder": "v1"
    },
    "staticData": null,
    "meta": {
      "templateCredsSetupCompleted": true
    },
    "pinData": null,
    "versionId": "231c0beb-67f5-4fca-ac63-de7cac7d6156",
    "activeVersionId": "231c0beb-67f5-4fca-ac63-de7cac7d6156",
    "versionCounter": 111,
    "triggerCount": 1,
    "shared": [
      {
        "updatedAt": "2025-12-17T14:18:50.701Z",
        "createdAt": "2025-12-17T14:18:50.701Z",
        "role": "workflow:owner",
        "workflowId": "8GCA2MiYJ0R64Ort",
        "projectId": "Pe0kjUniwpiBd4qS",
        "project": {
          "updatedAt": "2025-11-08T09:00:59.033Z",
          "createdAt": "2025-11-08T09:00:33.514Z",
          "id": "Pe0kjUniwpiBd4qS",
          "name": "admin admin <molodoiars@gmail.com>",
          "type": "personal",
          "icon": null,
          "description": null,
          "projectRelations": [
            {
              "updatedAt": "2025-11-08T09:00:33.514Z",
              "createdAt": "2025-11-08T09:00:33.514Z",
              "userId": "07806a46-b67e-4b6f-9ee2-8c3790bb1f7f",
              "projectId": "Pe0kjUniwpiBd4qS",
              "user": {
                "updatedAt": "2025-12-19T09:29:25.000Z",
                "createdAt": "2025-11-08T09:00:32.909Z",
                "id": "07806a46-b67e-4b6f-9ee2-8c3790bb1f7f",
                "email": "molodoiars@gmail.com",
                "firstName": "admin",
                "lastName": "admin",
                "personalizationAnswers": {
                  "version": "v4",
                  "personalization_survey_submitted_at": "2025-11-08T09:01:03.436Z",
                  "personalization_survey_n8n_version": "1.118.1"
                },
                "settings": {
                  "userActivated": true,
                  "easyAIWorkflowOnboarded": true,
                  "firstSuccessfulWorkflowId": "GbK4JsX36MFOisoQ",
                  "userActivatedAt": 1765198162186,
                  "npsSurvey": {
                    "waitingForResponse": true,
                    "ignoredCount": 1,
                    "lastShownAt": 1765550520070
                  }
                },
                "disabled": false,
                "mfaEnabled": false,
                "lastActiveAt": "0NaN-aN-aN",
                "isPending": false
              }
            }
          ]
        }
      }
    ],
    "tags": [],
    "activeVersion": {
      "updatedAt": "2025-12-19T09:23:22.251Z",
      "createdAt": "2025-12-19T09:23:22.251Z",
      "versionId": "231c0beb-67f5-4fca-ac63-de7cac7d6156",
      "workflowId": "8GCA2MiYJ0R64Ort",
      "nodes": [
        {
          "parameters": {
            "inputSource": "passthrough"
          },
          "type": "n8n-nodes-base.executeWorkflowTrigger",
          "typeVersion": 1.1,
          "position": [
            80,
            -208
          ],
          "id": "335a7169-9521-4b89-9a5c-e39fed785db3",
          "name": "When Executed by Another Workflow"
        },
        {
          "parameters": {
            "mode": "runOnceForEachItem",
            "jsCode": "const body = $json?.body ?? null;\nconst payload = (body && typeof body === \"object\") ? body : $json;\n\n// Promote webhook body fields to top-level so DB query / Gate logic can read them.\nconst merged = (body && typeof body === \"object\") ? { ...$json, ...body } : { ...$json };\n\nconst qRaw =\n  payload?.input?.question ??\n  payload?.question ??\n  payload?.text ??\n  payload?.telegram?.message?.text ??\n  \"\";\n\nconst question = String(qRaw).trim();\n\nconst kbRefRaw =\n  payload?.meta?.kb_ref ??\n  payload?.kb_ref ??\n  merged?.meta?.kb_ref ??\n  merged?.kb_ref ??\n  \"\";\n\nconst kb_ref = String(kbRefRaw).trim() || null;\n\nconst retrievalIn =\n  (payload?.retrieval && typeof payload.retrieval === \"object\") ? payload.retrieval :\n  (merged?.retrieval && typeof merged.retrieval === \"object\") ? merged.retrieval :\n  {};\n\nconst topKRaw = retrievalIn?.top_k ?? retrievalIn?.topK ?? payload?.top_k ?? payload?.topK ?? null;\nconst top_k = topKRaw == null ? 5 : Number(topKRaw);\n\nconst retrieval = {\n  ...retrievalIn,\n  top_k: Number.isFinite(top_k) && top_k > 0 ? top_k : 5,\n};\n\nconst chatIdRaw =\n  payload?.meta?.chat_id ??\n  payload?.chat_id ??\n  payload?.telegram?.message?.chat?.id ??\n  payload?.input?.user_id ??\n  merged?.meta?.chat_id ??\n  merged?.chat_id ??\n  merged?.telegram?.message?.chat?.id ??\n  merged?.input?.user_id ??\n  null;\n\nconst chat_id = chatIdRaw == null ? null : Number(chatIdRaw);\n\nconst request_id =\n  payload?.meta?.request_id ??\n  payload?.request_id ??\n  merged?.meta?.request_id ??\n  merged?.request_id ??\n  null;\n\nconst metaIn = (payload?.meta && typeof payload.meta === \"object\") ? payload.meta : (merged?.meta ?? {});\nconst inputIn = (payload?.input && typeof payload.input === \"object\") ? payload.input : (merged?.input ?? {});\n\nconst meta = {\n  ...metaIn,\n  kb_ref: metaIn?.kb_ref ?? kb_ref,\n};\n\nconst input = {\n  ...inputIn,\n  question: inputIn?.question ?? question,\n};\n\nreturn {\n  json: {\n    ...merged,\n    kb_ref,\n    retrieval,\n    meta,\n    input,\n    question,\n    chat_id: Number.isFinite(chat_id) ? chat_id : null,\n    request_id,\n  },\n};"
          },
          "type": "n8n-nodes-base.code",
          "typeVersion": 2,
          "position": [
            288,
            -208
          ],
          "id": "f66ded67-36ee-46d7-a77c-9e3a0d1058c6",
          "name": "KB — Input Normalize"
        },
        {
          "parameters": {
            "method": "POST",
            "url": "https://api.openai.com/v1/embeddings",
            "authentication": "predefinedCredentialType",
            "nodeCredentialType": "openAiApi",
            "sendBody": true,
            "specifyBody": "json",
            "jsonBody": "={\n  \"model\": \"text-embedding-3-small\",\n  \"input\": \"={{$json.question}}\"\n}\n",
            "options": {
              "timeout": 6000
            }
          },
          "type": "n8n-nodes-base.httpRequest",
          "typeVersion": 4.3,
          "position": [
            480,
            -208
          ],
          "id": "c55f5af8-78ed-49cb-ab4e-85bbfa9e1b92",
          "name": "OpenAI — Embeddings (HTTP)",
          "credentials": {
            "openAiApi": {
              "id": "j2qEVgJkVL5JMOK6",
              "name": "OpenAi account"
            }
          }
        },
        {
          "parameters": {
            "mode": "runOnceForEachItem",
            "jsCode": "const idx = $itemIndex;\nconst env = $items(\"KB — Input Normalize\")?.[idx]?.json ?? {};\nconst emb = $json.data?.[0]?.embedding;\nif (!emb) throw new Error(\"No embedding in OpenAI response\");\n\nreturn {\n  json: {\n    ...env,\n    query_embedding: emb,\n    meta: {\n      ...(env.meta ?? {}),\n      embedding_model: $json.model ?? \"text-embedding-3-small\",\n    },\n    timers: {\n      ...(env.timers ?? {}),\n      t_after_embed_ms: Date.now(),\n    },\n  },\n};\n"
          },
          "type": "n8n-nodes-base.code",
          "typeVersion": 2,
          "position": [
            656,
            -208
          ],
          "id": "469d3e35-5898-4aac-bf45-e70070791971",
          "name": "KB — Extract Query Embedding"
        },
        {
          "parameters": {
            "operation": "executeQuery",
            "query": "with p as (select $1::jsonb as j),\nscored as (\n  select\n    k.chunk_id, k.kb_ref, k.doc, k.section, k.chunk_index, k.content, k.source_url,\n    (k.embedding <-> (p.j->>'query_embedding')::vector) as distance,\n    row_number() over (order by k.embedding <-> (p.j->>'query_embedding')::vector) as rn,\n    coalesce((p.j->'retrieval'->>'top_k')::int, 5) as top_k\n  from public.kb_chunks k\n  cross join p\n  where k.kb_ref = coalesce(p.j->>'kb_ref', 'demo_support')\n)\nselect\n  chunk_id, kb_ref, doc, section, chunk_index, content, source_url,\n  distance,\n  1.0 / (1.0 + distance) as score\nfrom scored\nwhere rn <= top_k\norder by rn;\n",
            "options": {
              "connectionTimeout": 6,
              "queryReplacement": "={{ JSON.stringify($json) }}"
            }
          },
          "type": "n8n-nodes-base.postgres",
          "typeVersion": 2.6,
          "position": [
            816,
            -208
          ],
          "id": "64a7f4eb-5f32-4d8a-a622-5683390f199c",
          "name": "DB — Retrieve kb_chunks (topK)",
          "credentials": {
            "postgres": {
              "id": "QLUbzmxXFGSZ8LHx",
              "name": "AI_Support_Assistant"
            }
          }
        },
        {
          "parameters": {
            "jsCode": "// items = строки из DB (каждая строка — отдельный item)\nconst rows = items\n  .map(i => i.json)\n  .filter(r => r && r.content);\n\n// подтягиваем исходный Envelope (из ноды перед DB)\nconst env = $node[\"KB — Extract Query Embedding\"]?.json ?? {};\n\n// сортируем по score (чем больше — тем релевантнее)\nrows.sort((a, b) => (b.score ?? 0) - (a.score ?? 0));\n\nconst MAX_CHARS_PER_CHUNK = 1400;\n\nconst kb_sources = rows.map((r, idx) => ({\n  n: idx + 1,\n  chunk_id: r.chunk_id ?? null,\n  doc: r.doc ?? null,\n  section: r.section ?? null,\n  source_url: r.source_url ?? null,\n  score: r.score ?? null,\n  distance: r.distance ?? null,\n}));\n\nconst kb_context_text = rows.map((r, idx) => {\n  let content = String(r.content ?? \"\").trim();\n  if (content.length > MAX_CHARS_PER_CHUNK) {\n    content = content.slice(0, MAX_CHARS_PER_CHUNK) + \"…\";\n  }\n  const scoreStr = (r.score ?? 0).toFixed(3);\n  return `[${idx + 1}] doc=\"${r.doc ?? \"\"}\" section=\"${r.section ?? \"\"}\" score=${scoreStr}\n${content}`;\n}).join(\"\n\n\");\n\n// обновим retrieval в envelope\nconst top_score = rows[0]?.score ?? null;\n\nreturn [{\n  json: {\n    ...env,\n    retrieval: {\n      ...(env.retrieval ?? {}),\n      hits_count: rows.length,\n      top_score,\n      hits: kb_sources,\n      compiled_context: kb_context_text,\n    },\n    kb_context_text,\n    kb_sources,\n  }\n}];\n"
          },
          "type": "n8n-nodes-base.code",
          "typeVersion": 2,
          "position": [
            976,
            -208
          ],
          "id": "68396f44-aa53-49a4-aa60-c0539b11a3d3",
          "name": "KB — Build Context"
        },
        {
          "parameters": {
            "conditions": {
              "options": {
                "caseSensitive": true,
                "leftValue": "",
                "typeValidation": "strict",
                "version": 3
              },
              "conditions": [
                {
                  "id": "88b64325-a1e5-4c10-9b78-4446f2523d48",
                  "leftValue": "={{$json.decision.mode}}",
                  "rightValue": "ALLOW",
                  "operator": {
                    "type": "string",
                    "operation": "equals",
                    "name": "filter.operator.equals"
                  }
                }
              ],
              "combinator": "and"
            },
            "options": {}
          },
          "type": "n8n-nodes-base.if",
          "typeVersion": 2.3,
          "position": [
            1280,
            -208
          ],
          "id": "74ebcdcf-cb94-4c45-a065-f113fb1bf7a1",
          "name": "If"
        },
        {
          "parameters": {
            "jsCode": "const topScore =\n  Number($json?.retrieval?.top_score ?? $json?.retrieval?.topScore ?? 0);\n\nconst hitsCount =\n  Number($json?.retrieval?.hits_count\n    ?? $json?.retrieval?.hitsCount\n    ?? $json?.retrieval?.hits?.length\n    ?? $json?.hits_count\n    ?? 0);\n\nconst cfg = {\n  minHits: 1,\n  tClarify: 0.46, // T1\n  tAllow: 0.52    // T2\n};\n\nlet mode = \"NO_ANSWER\";\nlet reason = \"no_hits\";\n\nif (hitsCount < cfg.minHits) {\n  mode = \"NO_ANSWER\";\n  reason = \"no_hits\";\n} else if (topScore >= cfg.tAllow) {\n  mode = \"ALLOW\";\n  reason = \"ok\";\n} else if (topScore >= cfg.tClarify) {\n  mode = \"CLARIFY\";\n  reason = \"low_confidence\";\n} else {\n  mode = \"NO_ANSWER\";\n  reason = \"low_similarity\";\n}\n\n// For NON-ALLOW we immediately set a user-facing fallback message in output.answer\n// so the parent workflow can just Send Message + Log (no LLM call).\nlet output = $json.output ?? {};\nif (mode !== \"ALLOW\") {\n  if (mode === \"CLARIFY\") {\n    output.answer =\n      \"I found partially relevant info in the knowledge base, but I need a bit more detail to answer.\\n\\n\" +\n      \"What to clarify:\\n\" +\n      \"- Which exact section/process are you referring to (function/page/step name)?\\n\" +\n      \"- Which system/integration is this about (if there are multiple)?\";\n  } else {\n    output.answer =\n      \"I couldn’t find an answer in the knowledge base for this question.\\n\\n\" +\n      \"What you can do:\\n\" +\n      \"- Rephrase the question\\n\" +\n      \"- Add 1–2 details (feature/section name, error code, step in the process)\";\n  }\n  output.sources = [];\n}\n\nreturn [{\n  json: {\n    ...$json,\n    decision: { ...($json.decision ?? {}), mode, reason },\n    output\n  }\n}];\n"
          },
          "type": "n8n-nodes-base.code",
          "typeVersion": 2,
          "position": [
            1136,
            -208
          ],
          "id": "9d58433d-556d-42be-a6fa-c9093ed43d07",
          "name": "Gate Decide"
        },
        {
          "parameters": {
            "jsCode": "return items.map(item => {\n  const j = item.json;\n\n  const picked = Array.isArray(j.output?.picked_hits) ? j.output.picked_hits : [];\n  const sources = picked\n    .map((h, i) => ({\n      n: h.n ?? (i + 1),\n      chunk_id: h.chunk_id ?? null,\n      doc: h.doc ?? \"\",\n      section: h.section ?? \"\",\n      source_url: h.source_url ?? null,\n      score: Number(h.score ?? 0)\n    }))\n    .sort((a, b) => b.score - a.score)\n    .slice(0, 5);\n\n  j.output = j.output ?? {};\n  j.output.sources = sources;\n\n  return { json: j };\n});"
          },
          "type": "n8n-nodes-base.code",
          "typeVersion": 2,
          "position": [
            2320,
            -368
          ],
          "id": "9b4981b9-8eb8-41df-a431-27fe60e84531",
          "name": "KB — Build Sources"
        },
        {
          "parameters": {
            "jsCode": "return items.map(item => {\n  const j = item.json;\n\n  const mode = String(j.decision?.mode ?? \"NO_ANSWER\").toUpperCase();\n  const answerText = (j.output?.answer_text ?? \"\").trim();\n  const clarify = Array.isArray(j.output?.clarify) ? j.output.clarify : [];\n  const sources = Array.isArray(j.output?.sources) ? j.output.sources : [];\n\n  let text = \"\";\n\n  if (mode === \"ALLOW\") {\n    text = answerText || \"OK.\";\n\n    if (sources.length) {\n      const sourcesBlock =\n        \"\\n\\nSources:\\n\" +\n        sources.map((s, i) => {\n          const title = `${s.doc}${s.section ? \" — \" + s.section : \"\"}`;\n          const url = s.source_url ? `\\n${s.source_url}` : \"\";\n          return `${i + 1}) ${title}${url}`;\n        }).join(\"\\n\\n\");\n\n      text += sourcesBlock;\n    }\n  } else if (mode === \"CLARIFY\") {\n    const qs = clarify.length ? clarify : [\"Please уточни вопрос.\"];\n    text = \"I need a bit more info:\\n\" + qs.map(x => `- ${x}`).join(\"\\n\");\n  } else {\n    text = \"I couldn't find the answer in the Knowledge Base.\";\n  }\n\n  j.output = j.output ?? {};\n  j.output.reply_text = text;\n  j.output.answer = text; // backward-compat\n\n  return { json: j };\n});"
          },
          "type": "n8n-nodes-base.code",
          "typeVersion": 2,
          "position": [
            2528,
            -368
          ],
          "id": "fa683efe-1aa1-4fa4-b21a-91fa6d647d21",
          "name": "KB — Build Final Reply"
        },
        {
          "parameters": {
            "jsCode": "return items.map(item => {\n  const j = item.json;\n\n  delete j.query_embedding;\n\n  if (j.retrieval) {\n    delete j.retrieval.compiled_context;\n    if (Array.isArray(j.retrieval.hits)) {\n      j.retrieval.hits = j.retrieval.hits.map(h => {\n        const hh = { ...h };\n        delete hh.embedding;\n        delete hh.embedding_vector;\n        return hh;\n      });\n    }\n  }\n\n  if (j.llm) {\n    delete j.llm.raw_text;\n  }\n\n  if (j.output) {\n    delete j.output.picked_hits;\n  }\n\n  return { json: j };\n});"
          },
          "type": "n8n-nodes-base.code",
          "typeVersion": 2,
          "position": [
            2736,
            -368
          ],
          "id": "47875140-f5bc-4c07-bf3d-515fb90fdeb6",
          "name": "KB — Strip Heavy Fields"
        },
        {
          "parameters": {
            "jsCode": "return items.map(item => {\n  const j = item.json;\n\n  j.decision = j.decision ?? {};\n  j.output = j.output ?? {};\n\n  const mode = String(j.decision.mode ?? \"NO_ANSWER\").toUpperCase();\n\n  // Keep fallback responses citation-free (sources are attached only for ALLOW).\n  j.output.sources = [];\n\n  if (mode === \"CLARIFY\") {\n    if (!String(j.output.answer ?? \"\").trim()) {\n      j.output.answer =\n        \"I found partially relevant info in the knowledge base, but I need a bit more detail to answer.\\n\\n\" +\n        \"What to clarify:\\n\" +\n        \"- Which exact section/process are you referring to (function/page/step name)?\\n\" +\n        \"- Which system/integration is this about (if there are multiple)?\";\n    }\n  } else {\n    if (!String(j.output.answer ?? \"\").trim()) {\n      j.output.answer =\n        \"I couldn’t find an answer in the knowledge base for this question.\\n\\n\" +\n        \"What you can do:\\n\" +\n        \"- Rephrase the question\\n\" +\n        \"- Add 1–2 details (feature/section name, error code, step in the process)\";\n    }\n  }\n\n  return { json: j };\n});"
          },
          "type": "n8n-nodes-base.code",
          "typeVersion": 2,
          "position": [
            1472,
            -112
          ],
          "id": "834669b3-ef6b-4681-b943-bb1e62475ccc",
          "name": "Ops — Finalize Fallback"
        },
        {
          "parameters": {
            "jsCode": "return items.map(item => {\n  const j = item.json;\n\n  delete j.query_embedding;\n\n  // обычно при fallback контекст/источники тоже не нужны дальше\n  // (оставь, если хочешь видеть в логах)\n  // delete j.kb_context_text;\n  // delete j.kb_sources;\n\n  return { json: j };\n});"
          },
          "type": "n8n-nodes-base.code",
          "typeVersion": 2,
          "position": [
            1648,
            -112
          ],
          "id": "c6a318df-42ae-479a-94f5-c5135d155b04",
          "name": "Ops — Strip Heavy Fields"
        },
        {
          "parameters": {
            "modelId": {
              "__rl": true,
              "value": "gpt-4.1-mini",
              "mode": "list",
              "cachedResultName": "GPT-4.1-MINI"
            },
            "responses": {
              "values": [
                {
                  "role": "system",
                  "content": "={{ $json.prompt_text }}"
                },
                {
                  "content": "=Question:\n{{$json.input.question}}\n\nContext (KB chunks):\n{{$json.retrieval.compiled_context}}"
                }
              ]
            },
            "simplify": false,
            "builtInTools": {},
            "options": {
              "temperature": 0.2
            }
          },
          "type": "@n8n/n8n-nodes-langchain.openAi",
          "typeVersion": 2.1,
          "position": [
            1808,
            -368
          ],
          "id": "e0584636-4cf1-449b-ae85-c8775ea667b7",
          "name": "OpenAI — Answer v1",
          "retryOnFail": true,
          "credentials": {
            "openAiApi": {
              "id": "j2qEVgJkVL5JMOK6",
              "name": "OpenAi account"
            }
          },
          "onError": "continueRegularOutput"
        },
        {
          "parameters": {
            "jsCode": "const base = $node[\"Gate Decide\"].json;\n\nfunction stripCodeFences(s) {\n  return String(s ?? \"\")\n    .trim()\n    .replace(/^```(?:json)?\\s*/i, \"\")\n    .replace(/\\s*```$/i, \"\")\n    .trim();\n}\n\nfunction extractResponsesOutputText(resp) {\n  const out = resp?.output;\n  if (Array.isArray(out)) {\n    for (const item of out) {\n      if (item?.type === \"message\" && Array.isArray(item?.content)) {\n        for (const c of item.content) {\n          if (c?.type === \"output_text\" && typeof c?.text === \"string\" && c.text.trim()) {\n            return c.text.trim();\n          }\n        }\n        const t2 = item.content?.[0]?.text;\n        if (typeof t2 === \"string\" && t2.trim()) return t2.trim();\n      }
    }
  }
  return \"\";
}

function extractChatCompletionsText(resp) {
  const cc =
    resp?.choices?.[0]?.message?.content ??
    resp?.data?.choices?.[0]?.message?.content ??
    resp?.message?.content;
  if (typeof cc === \"string\" && cc.trim()) return cc.trim();
  return \"\";
}

function extractResponseText(resp) {
  const t = extractResponsesOutputText(resp);
  if (t) return t;
  const cc = extractChatCompletionsText(resp);
  if (cc) return cc;
  if (typeof resp === \"string\" && resp.trim()) return resp.trim();
  return \"\";
}

function parseJsonLoose(raw) {
  const s = stripCodeFences(raw);
  try {
    const p = JSON.parse(s);
    if (typeof p === \"string\") return JSON.parse(stripCodeFences(p));
    return p;
  } catch {}
  const m = s.match(/\{[\s\S]*\}/);
  if (m) {
    try {
      return JSON.parse(m[0]);
    } catch {}
  }
  return null;
}

function normalizeMode(m) {
  const x = String(m ?? \"\").trim().toUpperCase();
  if (x === \"ANSWER\") return \"ALLOW\";
  if (x === \"ALLOW\") return \"ALLOW\";
  if (x === \"CLARIFY\") return \"CLARIFY\";
  if (x === \"NO_ANSWER\" || x === \"NOANSWER\" || x === \"NO-ANSWER\") return \"NO_ANSWER\";
  return \"\";
}

function ensureArray(x) {
  if (Array.isArray(x)) return x;
  if (typeof x === \"string\" && x.trim()) return [x.trim()];
  return [];
}

function pickHits({ parsed, answerText, hits, topScore }) {
  const byN = new Map();
  const byId = new Map();
  for (const h of hits) {
    if (h?.n != null) byN.set(Number(h.n), h);
    if (h?.chunk_id) byId.set(String(h.chunk_id), h);
  }

  const explicit = parsed?.sources;
  if (Array.isArray(explicit) && explicit.length) {
    const picked = [];
    for (const x of explicit) {
      if (typeof x === \"number\" || /^\d+$/.test(String(x))) {
        const h = byN.get(Number(x));
        if (h) picked.push(h);
      } else {
        const h = byId.get(String(x));
        if (h) picked.push(h);
      }
    }
    if (picked.length) return picked;
  }

  const cited = new Set();
  const re = /\[(\d+)\]/g;
  let m;
  while ((m = re.exec(String(answerText ?? \"\"))) !== null) cited.add(Number(m[1]));
  if (cited.size) return [...cited].map((n) => byN.get(n)).filter(Boolean);

  const ts = typeof topScore === \"number\" ? topScore : Number(hits?.[0]?.score ?? 0);
  const minScore = ts * 0.9;
  return hits.filter((h) => Number(h?.score ?? 0) >= minScore);
}

// вытаскиваем текст первого источника из kb_context_text
function extractTopSourceText(kbContext) {
  const s = String(kbContext ?? \"\");
  const m = s.match(/\[1\][^\n]*\n([\s\S]*?)(?:\n\n\[\d+\]|\s*$)/);
  if (!m) return \"\";
  return String(m[1] ?? \"\").trim();
}

function extractModel(resp) {
  const direct = [resp?.model, resp?.data?.model, resp?.info?.model, resp?.metadata?.model];
  for (const m of direct) {
    if (typeof m === \"string\" && m.trim()) return m.trim();
  }

  const choices = resp?.choices ?? resp?.data?.choices;
  if (Array.isArray(choices)) {
    for (const c of choices) {
      const m = c?.model ?? c?.message?.model ?? c?.completion?.model;
      if (typeof m === \"string\" && m.trim()) return m.trim();
    }
  }

  const nested = resp?.response ?? resp?.result ?? resp?.body;
  if (nested && nested !== resp) {
    const nestedModel = extractModel(nested);
    if (nestedModel) return nestedModel;
  }

  return \"\";
}

// ====== main ======
const raw = extractResponseText($json);
const parsed = parseJsonLoose(raw);

const hits = base?.kb_sources ?? base?.retrieval?.hits ?? [];
const topScore = base?.retrieval?.top_score ?? Number(hits?.[0]?.score ?? 0);

// gate decision (retrieval)
const gateMode = String(base?.decision?.mode ?? \"ALLOW\").toUpperCase();

// llm mode
let llmMode = normalizeMode(parsed?.mode);
let llmReason = \"ok\";

if (!llmMode) {
  llmMode = \"CLARIFY\";
  llmReason = \"llm_parse_error\";
}

let finalMode = llmMode;
let finalReason = llmReason;

let answerText = \"\";
let clarify = [];

// если gate НЕ ALLOW — просто не трогаем (как правило LLM тут вообще не должен вызываться)
if (gateMode !== \"ALLOW\") {
  finalMode = gateMode;
  finalReason = base?.decision?.reason ?? \"ok\";
}

// если gate ALLOW — решаем, что показывать пользователю
if (gateMode === \"ALLOW\") {
  if (llmMode === \"ALLOW\") {
    answerText = String(parsed?.answer ?? \"\").trim();
  } else if (llmMode === \"CLARIFY\") {
    clarify = ensureArray(parsed?.clarify);
  } else if (llmMode === \"NO_ANSWER\") {
    answerText = String(parsed?.answer ?? \"\").trim();
  }

  // OVERRIDE: если LLM сказал CLARIFY/NO_ANSWER, но top_score высокий и top source явно отвечает — отвечаем
  const FORCE_ANSWER_SCORE = 0.5; // “high enough” threshold
  if ((llmMode === \"CLARIFY\" || llmMode === \"NO_ANSWER\") && hits.length && topScore >= FORCE_ANSWER_SCORE) {
    const topText = extractTopSourceText(base?.kb_context_text);
    if (topText) {
      finalMode = \"ALLOW\";
      finalReason = \"llm_override_high_score\";
      answerText = topText;
      clarify = [];
    }
  }

  // если ALLOW, но пусто — переводим в CLARIFY как safe fallback
  if (finalMode === \"ALLOW\" && !String(answerText).trim()) {
    finalMode = \"CLARIFY\";
    finalReason = \"llm_empty_answer\";
    clarify = [\"What exact access do you need (system/app) and for which role?\"];
  }

  // если CLARIFY и пусто — дефолт
  if (finalMode === \"CLARIFY\" && (!Array.isArray(clarify) || !clarify.length)) {
    clarify = [\"What exact access do you need (system/app) and for which role?\"];
  }

  // если NO_ANSWER и пусто — дефолт
  if (finalMode === \"NO_ANSWER\" && !String(answerText).trim()) {
    answerText = \"I couldn't find the answer in the Knowledge Base.\";
  }
}

// sources только для ALLOW
let picked_hits = [];
if (finalMode === \"ALLOW\") {
  picked_hits = pickHits({ parsed, answerText, hits, topScore });
}

const detectedModel = extractModel($json);
const nextMeta = { ...(base.meta ?? {}) };

if (detectedModel) nextMeta.chat_model = detectedModel;
if (nextMeta.prompt_version == null && base?.meta?.prompt_version != null) {
  nextMeta.prompt_version = base.meta.prompt_version;
}

return [
  {
    json: {
      ...base,
      meta: nextMeta,
      // ✅ теперь decision = финальный режим (консистентно с ответом)
      decision: {
        ...(base.decision ?? {}),
        mode: finalMode,
        reason: finalReason,
      },
      output: {
        ...(base.output ?? {}),
        mode: finalMode,
        answer_text: finalMode === \"ALLOW\" ? String(answerText).trim() : \"\",
        clarify: finalMode === \"CLARIFY\" ? clarify : [],
        picked_hits,
      },
      llm: {
        raw_text: raw,
        parsed: parsed ?? null,
        model: detectedModel || null,
      },
    },
  },
];"
          },
          "type": "n8n-nodes-base.code",
          "typeVersion": 2,
          "position": [
            2112,
            -368
          ],
          "id": "c7c02f37-1142-481b-aad9-8d3b4e3a6346",
          "name": "KB — Parse Answer"
        },
        {
          "parameters": {
            "url": "https://raw.githubusercontent.com/ArsenYoung/ai-support-rag-assistant/main/prompts/answer.md",
            "options": {
              "response": {
                "response": {
                  "responseFormat": "text"
                }
              }
            }
          },
          "type": "n8n-nodes-base.httpRequest",
          "typeVersion": 4.3,
          "position": [
            1472,
            -368
          ],
          "id": "2d8d32e9-1a40-48bd-aa03-fe613e66b903",
          "name": "Prompt — Load answer.md"
        },
        {
          "parameters": {
            "jsCode": "function fnv1a32(str) {\n  let h = 0x811c9dc5;\n  for (let i = 0; i < str.length; i++) {\n    h ^= str.charCodeAt(i);\n    h = Math.imul(h, 0x01000193);\n  }\n  return (\"00000000\" + (h >>> 0).toString(16)).slice(-8);\n}\n\nconst prompt_text = ($json.body ?? $json.data ?? $json.prompt ?? \"\").toString();\nif (!prompt_text.trim()) {\n  throw new Error(\"PROMPT_LOAD_ERROR: empty prompt_text\");\n}\n\nreturn [{\n  json: {\n    ...$json,\n    prompt_text,\n    meta: {\n      ...( $json.meta || {} ),\n      prompt_version: \"v1\",\n      prompt_hash: \"fnv1a32:\" + fnv1a32(prompt_text),\n    }\n  }\n}];"
          },
          "type": "n8n-nodes-base.code",
          "typeVersion": 2,
          "position": [
            1648,
            -368
          ],
          "id": "3fc3f75e-4d6d-4716-8342-27070cb38955",
          "name": "Prompt — Attach Meta"
        },
        {
          "parameters": {
            "workflowId": {
              "__rl": true,
              "value": "Z8yQeXYTQSnRkl3M",
              "mode": "list",
              "cachedResultUrl": "/workflow/Z8yQeXYTQSnRkl3M",
              "cachedResultName": "Ops — Log chat_turn"
            },
            "workflowInputs": {
              "mappingMode": "defineBelow",
              "value": {},
              "matchingColumns": [],
              "schema": [],
              "attemptToConvertTypes": false,
              "convertFieldsToString": true
            },
            "options": {
              "waitForSubWorkflow": false
            }
          },
          "type": "n8n-nodes-base.executeWorkflow",
          "typeVersion": 1.3,
          "position": [
            3264,
            -112
          ],
          "id": "32ac22c0-e821-4108-89c1-f264ae04f050",
          "name": "Call 'Ops — Log chat_turn'"
        },
        {
          "parameters": {
            "httpMethod": "POST",
            "path": "kb/answer",
            "responseMode": "responseNode",
            "options": {}
          },
          "type": "n8n-nodes-base.webhook",
          "typeVersion": 2.1,
          "position": [
            80,
            -368
          ],
          "id": "cb195b64-c1b5-470d-b6a3-536c65f6b1db",
          "name": "Webhook",
          "webhookId": "d3de4c60-5f27-430d-aa33-22ac35c89e58"
        },
        {
          "parameters": {
            "options": {
              "responseCode": 200
            }
          },
          "type": "n8n-nodes-base.respondToWebhook",
          "typeVersion": 1.5,
          "position": [
            2896,
            -208
          ],
          "id": "6b568123-beef-4e31-9c4f-afc8ade73689",
          "name": "Respond 200"
        }
      ],
      "connections": {
        "When Executed by Another Workflow": {
          "main": [
            [
              {
                "node": "KB — Input Normalize",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "KB — Input Normalize": {
          "main": [
            [
              {
                "node": "OpenAI — Embeddings (HTTP)",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "OpenAI — Embeddings (HTTP)": {
          "main": [
            [
              {
                "node": "KB — Extract Query Embedding",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "KB — Extract Query Embedding": {
          "main": [
            [
              {
                "node": "DB — Retrieve kb_chunks (topK)",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "DB — Retrieve kb_chunks (topK)": {
          "main": [
            [
              {
                "node": "KB — Build Context",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "KB — Build Context": {
          "main": [
            [
              {
                "node": "Gate Decide",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "If": {
          "main": [
            [
              {
                "node": "Prompt — Load answer.md",
                "type": "main",
                "index": 0
              }
            ],
            [
              {
                "node": "Ops — Finalize Fallback",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "Gate Decide": {
          "main": [
            [
              {
                "node": "If",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "KB — Build Sources": {
          "main": [
            [
              {
                "node": "KB — Build Final Reply",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "KB — Build Final Reply": {
          "main": [
            [
              {
                "node": "KB — Strip Heavy Fields",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "Ops — Finalize Fallback": {
          "main": [
            [
              {
                "node": "Ops — Strip Heavy Fields",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "OpenAI — Answer v1": {
          "main": [
            [
              {
                "node": "KB — Parse Answer",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "KB — Parse Answer": {
          "main": [
            [
              {
                "node": "KB — Build Sources",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "Prompt — Load answer.md": {
          "main": [
            [
              {
                "node": "Prompt — Attach Meta",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "Prompt — Attach Meta": {
          "main": [
            [
              {
                "node": "OpenAI — Answer v1",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "Ops — Strip Heavy Fields": {
          "main": [
            [
              {
                "node": "Call 'Ops — Log chat_turn'",
                "type": "main",
                "index": 0
              },
              {
                "node": "Respond 200",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "KB — Strip Heavy Fields": {
          "main": [
            [
              {
                "node": "Call 'Ops — Log chat_turn'",
                "type": "main",
                "index": 0
              },
              {
                "node": "Respond 200",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "Webhook": {
          "main": [
            [
              {
                "node": "KB — Input Normalize",
                "type": "main",
                "index": 0
              }
            ]
          ]
        }
      },
      "authors": "admin admin",
      "name": null,
      "description": null
    }
  }
}]